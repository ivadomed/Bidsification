{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 constrasts model\n",
    "\n",
    "This notebook load, preprocess the data and train a first modèle to predict if a 2 image is T1w or T2w.\n",
    "The Notebook form helps running and testing fast before coding the final structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from monai.data import Dataset, DataLoader, CacheDataset\n",
    "import torchvision.models as models\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    RandScaleCrop,\n",
    "    RandFlip,\n",
    "    RandRotate90,\n",
    "    RandRotate,\n",
    "    RandShiftIntensity,\n",
    "    ToTensor,\n",
    "    RandSpatialCrop,\n",
    "    LoadImage,\n",
    "    SqueezeDim,\n",
    "    RandRotate,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import monai\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for T1w, T2w, and DWI files in data//data-multi-subject// ...\n",
      "Found 267 T1w files and 267 T2w files.\n"
     ]
    }
   ],
   "source": [
    "# this cell aims at extracting the list of path relevant for the first model test which takes T1w T2w adn DWI as image\n",
    "\n",
    "base_dir=\"data//data-multi-subject//\"\n",
    "\n",
    "desired_extension = \".json\"\n",
    "\n",
    "# Initialize lists to store the relative paths for T1w, T2w, and DWI files\n",
    "t1w_file_paths = []\n",
    "t2w_file_paths = []\n",
    "\n",
    "print(\"Searching for T1w, T2w, and DWI files in\", base_dir, \"...\")\n",
    "\n",
    "# Traverse the directory structure\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    # Exclude the \"derivatives\" subfolder\n",
    "    if \"derivatives\" in dirs:\n",
    "        dirs.remove(\"derivatives\")\n",
    "    for file in files:\n",
    "        # Check if the file name contains the desired names\n",
    "        if \"T1w\" in file and file.endswith(desired_extension):\n",
    "            # Get the relative path of the T1w file\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), base_dir)\n",
    "            # Remove the file extension\n",
    "            relative_path = os.path.splitext(base_dir + relative_path)[0] + \".nii.gz\"\n",
    "            # Append the relative path to the T1w file paths list\n",
    "            t1w_file_paths.append(relative_path)\n",
    "        elif \"T2w\" in file and file.endswith(desired_extension):\n",
    "            # Get the relative path of the T2w file\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), base_dir)\n",
    "            # Remove the file extension\n",
    "            relative_path = os.path.splitext(relative_path)[0] + \".nii.gz\"\n",
    "            # Append the relative path to the T2w file paths list\n",
    "            t2w_file_paths.append(base_dir + relative_path)\n",
    "\n",
    "#t1w_file_paths = t1w_file_paths[:20]\n",
    "#t2w_file_paths = t2w_file_paths[:20]\n",
    "\n",
    "print(\"Found\", len(t1w_file_paths), \"T1w files and\", len(t2w_file_paths), \"T2w files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation sets\n",
    "\n",
    "# build a dataset with a colmn \"file path\" wich contiains the paths listed in both t1w_file_paths and t2w_file_paths\n",
    "path_data = pd.DataFrame({\"image_path\" : t1w_file_paths + t2w_file_paths, \"labels\" : len(t1w_file_paths) * [0] + len(t2w_file_paths) * [1]})\n",
    "\n",
    "train_data, val_data = train_test_split(path_data, test_size=0.2, random_state=0)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class Dataset_2D(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.data = {\"paths\" : paths, \"labels\" : labels}\n",
    "        self.transform = transform\n",
    "        self.length = len(self.data[\"paths\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.data[\"paths\"][index]\n",
    "        label = [0,1] if self.data[\"labels\"][index] else [1,0]\n",
    "        if self.transform:\n",
    "            image = self.transform(path)\n",
    "            dim_to_squeeze = int(np.random.choice([0,1,2]))\n",
    "            roi_min = np.array([30, 30, 30])\n",
    "            roi_max = np.array([-1, -1, -1])\n",
    "            roi_min[dim_to_squeeze] = 1\n",
    "            roi_max[dim_to_squeeze] = 1\n",
    "            image = RandSpatialCrop(roi_min,  max_roi_size = roi_max, random_size=True, random_center=True)(image)\n",
    "            image = SqueezeDim(dim=dim_to_squeeze + 1)(image)\n",
    "            # Convert to tensor\n",
    "            image = ToTensor()(image)\n",
    "            # add a dimension to the image, for exemple [1, 256, 256] -> [1, 1, 256, 256]\n",
    "            image = image.unsqueeze(0)\n",
    "\n",
    "        # convert label list to tensor with shape [1,2]\n",
    "        label = torch.tensor([label])\n",
    "        return image, label\n",
    "    \n",
    "# use monai to define the transforms for data augmentation\n",
    "# perform the following transformations : rotation (random between +3° and -3°), flipping (random between 0°,  90 °, 180° and 270°), cropping (Random size, random place) and shifting (random shift)\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImage(image_only=True, ensure_channel_first=True),\n",
    "        RandRotate90(prob=0.5),\n",
    "        RandFlip(prob=0.5),\n",
    "        ScaleIntensity(),\n",
    "        RandShiftIntensity(offsets=0.1, prob=0.5),\n",
    "        RandRotate(range_x=3, range_y=3, range_z=3, prob=0.2),\n",
    "\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImage(image_only=True, ensure_channel_first=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the custom datasets\n",
    "train_dataset = Dataset_2D(\n",
    "    paths=train_data['image_path'],\n",
    "    labels=train_data['labels'],\n",
    "    transform=train_transforms,\n",
    ")\n",
    "\n",
    "val_dataset = Dataset_2D(\n",
    "    paths=val_data['image_path'],\n",
    "    labels=val_data['labels'],\n",
    "    transform=val_transforms,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 92\n",
      "torch.Size([1, 1, 104, 44]) tensor([[1, 0]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get an image\n",
    "i = np.random.randint(0, train_dataset.length)\n",
    "print(\"Index:\", i)\n",
    "image, label = train_dataset[i]\n",
    "print(image.shape, label)\n",
    "\n",
    "#show image in black and white\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image[0, 0], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metatensor([[[[ 1.,  0.,  1.,  ...,  4.,  3.,  6.],\n",
      "          [ 1.,  1.,  1.,  ...,  3.,  4.,  5.],\n",
      "          [ 2.,  2.,  1.,  ...,  2.,  4.,  2.],\n",
      "          ...,\n",
      "          [ 0.,  1.,  0.,  ..., 19., 25., 24.],\n",
      "          [ 1.,  2.,  2.,  ..., 21., 19., 15.],\n",
      "          [ 1.,  2.,  2.,  ..., 10., 16., 20.]]]])\n",
      "metatensor([[0.4944, 0.6077]], grad_fn=<AliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class ResNet18SingleChannel(nn.Module):\n",
    "    # Define the ResNet18 model with a single image channel and an output value between 0 and 1\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet18SingleChannel, self).__init__()\n",
    "        # Load the pre-trained ResNet18 model\n",
    "        resnet = models.resnet18(pretrained=False)\n",
    "        # Modify the first convolutional layer to take a single channel input\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        # Modify the final fully connected layer to output a single value\n",
    "\n",
    "        self.resnet = resnet\n",
    "\n",
    "        #final fc to go from [batch_size, 1000] to [batch_size, num_classes]\n",
    "        self.fc = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet18SingleChannel(num_classes=2).to(device)\n",
    "\n",
    "image, label = train_dataset[0]\n",
    "print(image)\n",
    "output = model.forward(image.to(device))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100\n",
      "0\n",
      "torch.Size([1, 1, 91, 33])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1, 126, 274])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1, 271, 54])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1, 48, 226])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1, 92, 192])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1, 33, 297])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1, 36, 122])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1, 30, 220])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1, 108, 255])\n",
      "torch.Size([1, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     model, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m, in \u001b[0;36mtraining_one_epoch\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(index)\n\u001b[1;32m---> 16\u001b[0m image, label \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m image, label \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mDataset_2D.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     13\u001b[0m label \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m][index] \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 15\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     dim_to_squeeze \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m     17\u001b[0m     roi_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m30\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\monai\\transforms\\compose.py:335\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threading\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lazy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    334\u001b[0m     _lazy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[1;32m--> 335\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\monai\\transforms\\compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[1;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threading:\n\u001b[0;32m    110\u001b[0m         _transform \u001b[38;5;241m=\u001b[39m deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[1;32m--> 111\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m data \u001b[38;5;241m=\u001b[39m apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name\u001b[38;5;241m=\u001b[39mlog_stats)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\monai\\transforms\\transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[1;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m map_items:\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m MONAIEnvVars\u001b[38;5;241m.\u001b[39mdebug():\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\monai\\transforms\\transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[1;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m unpack_parameters:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transform(data, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\monai\\transforms\\io\\array.py:282\u001b[0m, in \u001b[0;36mLoadImage.__call__\u001b[1;34m(self, filename, reader)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot find a suitable reader for file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Please install the reader libraries, see also the installation instructions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   The current registered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreaders\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m     )\n\u001b[0;32m    281\u001b[0m img_array: NdarrayOrTensor\n\u001b[1;32m--> 282\u001b[0m img_array, meta_data \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m img_array \u001b[38;5;241m=\u001b[39m convert_to_dst_type(img_array, dst\u001b[38;5;241m=\u001b[39mimg_array, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(meta_data, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\monai\\data\\image_reader.py:938\u001b[0m, in \u001b[0;36mNibabelReader.get_data\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    936\u001b[0m header[MetaKeys\u001b[38;5;241m.\u001b[39mSPATIAL_SHAPE] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_spatial_shape(i)\n\u001b[0;32m    937\u001b[0m header[MetaKeys\u001b[38;5;241m.\u001b[39mSPACE] \u001b[38;5;241m=\u001b[39m SpaceKeys\u001b[38;5;241m.\u001b[39mRAS\n\u001b[1;32m--> 938\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_array_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msqueeze_non_spatial_dims:\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape), \u001b[38;5;28mlen\u001b[39m(header[MetaKeys\u001b[38;5;241m.\u001b[39mSPATIAL_SHAPE]), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\monai\\data\\image_reader.py:1012\u001b[0m, in \u001b[0;36mNibabelReader._get_array_data\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_array_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m    Get the raw array data of the image, converted to Numpy array.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \n\u001b[0;32m   1011\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nibabel\\arrayproxy.py:457\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nibabel\\arrayproxy.py:424\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[1;34m(self, dtype, slicer)\u001b[0m\n\u001b[0;32m    422\u001b[0m     scl_inter \u001b[38;5;241m=\u001b[39m scl_inter\u001b[38;5;241m.\u001b[39mastype(use_dtype)\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_unscaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslicer\u001b[49m\u001b[43m)\u001b[49m, scl_slope, scl_inter)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    426\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m scaled\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mpromote_types(scaled\u001b[38;5;241m.\u001b[39mdtype, dtype), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nibabel\\arrayproxy.py:394\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[1;34m(self, slicer)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canonical_slicers(slicer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m==\u001b[39m canonical_slicers(\n\u001b[0;32m    391\u001b[0m     (), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    392\u001b[0m ):\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 394\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fileslice(\n\u001b[0;32m    404\u001b[0m         fileobj,\n\u001b[0;32m    405\u001b[0m         slicer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m         lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock,\n\u001b[0;32m    411\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nibabel\\volumeutils.py:465\u001b[0m, in \u001b[0;36marray_from_file\u001b[1;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(infile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    464\u001b[0m     data_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(n_bytes)\n\u001b[1;32m--> 465\u001b[0m     n_read \u001b[38;5;241m=\u001b[39m \u001b[43minfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m     needs_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\gzip.py:494\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_member \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT_BUFFER_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(buf, size)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\gzip.py:88\u001b[0m, in \u001b[0;36m_PaddedFile.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length:\n\u001b[0;32m     90\u001b[0m         read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from monai.losses import DiceLoss\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Define the training loop\n",
    "def training_one_epoch(model):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    queue_line = np.arange(train_dataset.length)\n",
    "    np.random.shuffle(queue_line) \n",
    "    index=0\n",
    "    for i in queue_line:\n",
    "        if index%200 == 0:\n",
    "            print(index)\n",
    "        image, label = train_dataset[i]\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        print(image.shape)\n",
    "        print(label.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        index+=1\n",
    "    return model, running_loss / len(train_dataset)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs}\")\n",
    "    model, train_loss = training_one_epoch(model)\n",
    "    print(f\"Epoch {epoch + 1} training loss: {train_loss}\")\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f\"model_{epoch + 1}.pth\")\n",
    "#save model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metatensor([[0.6818, 0.3297]])\n",
      "metatensor([[0.6832, 0.3287]])\n",
      "metatensor([[0.0154, 0.9768]])\n",
      "metatensor([[0.6825, 0.3292]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\monai\\transforms\\utility\\array.py:597: UserWarning: After SqueezeDim, img.affine is ill-posed: \n",
      "tensor([[ 0.0000e+00,  4.3468e-02,  9.9905e-01, -5.6091e+01],\n",
      "        [-1.0002e+00,  0.0000e+00,  0.0000e+00,  9.5959e+01],\n",
      "        [ 0.0000e+00,  9.9928e-01, -4.3459e-02,  2.9378e+01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
      "       dtype=torch.float64).\n",
      "  warnings.warn(f\"After SqueezeDim, img.affine is ill-posed: \\n{img.affine}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metatensor([[0.6833, 0.3286]])\n",
      "metatensor([[0.6825, 0.3292]])\n",
      "metatensor([[0.6835, 0.3283]])\n",
      "metatensor([[0.6839, 0.3280]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\monai\\transforms\\utility\\array.py:597: UserWarning: After SqueezeDim, img.affine is ill-posed: \n",
      "tensor([[ 0.0000e+00,  2.2680e-02,  7.9968e-01, -2.6430e+01],\n",
      "        [-8.0000e-01,  0.0000e+00,  0.0000e+00, -4.8012e+01],\n",
      "        [ 0.0000e+00,  7.9968e-01, -2.2680e-02, -8.5927e+01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
      "       dtype=torch.float64).\n",
      "  warnings.warn(f\"After SqueezeDim, img.affine is ill-posed: \\n{img.affine}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metatensor([[0.6819, 0.3295]])\n",
      "metatensor([[0.6829, 0.3290]])\n",
      "metatensor([[0.6838, 0.3280]])\n",
      "metatensor([[0.6838, 0.3280]])\n",
      "metatensor([[0.6836, 0.3282]])\n",
      "metatensor([[0.6829, 0.3290]])\n",
      "metatensor([[0.6839, 0.3279]])\n",
      "metatensor([[0.6832, 0.3287]])\n",
      "metatensor([[0.6836, 0.3283]])\n",
      "metatensor([[0.6836, 0.3282]])\n",
      "metatensor([[0.6833, 0.3286]])\n",
      "metatensor([[0.6839, 0.3279]])\n",
      "metatensor([[0.6838, 0.3281]])\n",
      "metatensor([[0.6826, 0.3290]])\n",
      "metatensor([[0.6819, 0.3296]])\n",
      "metatensor([[0.6831, 0.3287]])\n",
      "metatensor([[0.6814, 0.3304]])\n",
      "metatensor([[0.6832, 0.3286]])\n",
      "metatensor([[0.6838, 0.3280]])\n",
      "metatensor([[0.6833, 0.3285]])\n",
      "metatensor([[0.6842, 0.3277]])\n",
      "metatensor([[0.0055, 0.9951]])\n",
      "metatensor([[0.6838, 0.3280]])\n",
      "metatensor([[0.6840, 0.3279]])\n",
      "metatensor([[0.6839, 0.3279]])\n",
      "metatensor([[0.6837, 0.3281]])\n",
      "metatensor([[0.6839, 0.3279]])\n",
      "metatensor([[0.6841, 0.3278]])\n",
      "metatensor([[0.6839, 0.3279]])\n",
      "metatensor([[0.6814, 0.3305]])\n",
      "metatensor([[0.6826, 0.3290]])\n",
      "metatensor([[0.6829, 0.3290]])\n",
      "metatensor([[0.6829, 0.3288]])\n",
      "metatensor([[0.6837, 0.3281]])\n",
      "metatensor([[0.6829, 0.3289]])\n",
      "metatensor([[0.6835, 0.3283]])\n",
      "metatensor([[0.6827, 0.3289]])\n",
      "metatensor([[0.6823, 0.3294]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\monai\\transforms\\utility\\array.py:597: UserWarning: After SqueezeDim, img.affine is ill-posed: \n",
      "tensor([[ 0.0000e+00,  2.8356e-02,  9.9960e-01,  7.6665e+00],\n",
      "        [-1.0002e+00,  0.0000e+00,  0.0000e+00, -4.6280e+01],\n",
      "        [ 0.0000e+00,  9.9982e-01, -2.8350e-02,  4.6102e+01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
      "       dtype=torch.float64).\n",
      "  warnings.warn(f\"After SqueezeDim, img.affine is ill-posed: \\n{img.affine}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metatensor([[0.6829, 0.3288]])\n",
      "metatensor([[0.6831, 0.3287]])\n",
      "metatensor([[0.6829, 0.3290]])\n",
      "metatensor([[0.6833, 0.3286]])\n",
      "metatensor([[0.6831, 0.3287]])\n",
      "metatensor([[0.6841, 0.3278]])\n",
      "metatensor([[0.6828, 0.3291]])\n",
      "metatensor([[0.6830, 0.3288]])\n",
      "Validation accuracy: 0.4953271028037383\n"
     ]
    }
   ],
   "source": [
    "# Assess the model accuracy on the validation set\n",
    "\n",
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(dataset.length):\n",
    "            image, label = dataset[i]\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            outputs = model(image)\n",
    "            predicted = torch.argmax(outputs)\n",
    "            if i % 2 == 0:\n",
    "                print(outputs)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == torch.argmax(label)).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "accuracy = evaluate_model(model, val_dataset)\n",
    "print(f\"Validation accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
